{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYb40dvGI1FqEuLnllPm6E"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Tokenizers**\n",
        "For this Colab session, we explore the world of Tokenizers\n",
        "\n",
        "You can run this notebook on a free CPU, or locally on your box if you prefer."
      ],
      "metadata": {
        "id": "H3vDoIhsMBF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the necessary libraries"
      ],
      "metadata": {
        "id": "xW86pda6YDsX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgt1i6uEKw78"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get my Hugging Face token from a variable called `userdata`, then log in to Hugging Face with it — and also save that login so I can use Git commands with Hugging Face repos."
      ],
      "metadata": {
        "id": "HU08scRAYAAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "EAirNJOaME_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and load the tokenizer that works with the Meta LLaMA 3.1 8B model, and trust any custom code provided with that model."
      ],
      "metadata": {
        "id": "-x77RFv5XsF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "OOsbDFHmQafF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take my sentence and use the tokenizer to turn it into a list of numbers that a language model can understand"
      ],
      "metadata": {
        "id": "9Ivdw6aBYYNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "Hyzz5YOfQa-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the length of the tokens"
      ],
      "metadata": {
        "id": "G4gRIiC7Ye-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "l0Clr-RTYLxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding the tokens"
      ],
      "metadata": {
        "id": "yk_k9lcJaBD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "B7Hf06Y_Yc7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the list of token IDs and convert them back into human-readable text"
      ],
      "metadata": {
        "id": "ON6BhBXNcu4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "b2X8ltAdckb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show me the entire list of words or pieces the tokenizer understands, along with the number assigned to each one."
      ],
      "metadata": {
        "id": "KrAZd8-cdBRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab"
      ],
      "metadata": {
        "id": "8dlawCOvck5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show me just the extra words or tokens I’ve added to the tokenizer — not the original built-in ones"
      ],
      "metadata": {
        "id": "Lmcipqj6dPHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "DdHTntHDc3wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instruct variants of models**\n",
        "\n",
        "Many models have a variant that has been trained for use in Chats.  \n",
        "These are typically labelled with the word \"Instruct\" at the end.  \n",
        "They have been trained to expect prompts with a particular format that includes system, user and assistant prompts.  \n",
        "\n",
        "There is a utility method `apply_chat_template` that will convert from the messages list format we are familiar with, into the right input prompt for this model."
      ],
      "metadata": {
        "id": "yTrlzfQSdWjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and use the tokenizer designed for the LLaMA 3.1 8B Instruct model, and allow it to run any special code it needs to work correctly"
      ],
      "metadata": {
        "id": "0bAgm-RiduVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "3F4X-XnVdZ4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a conversation between the user and assistant, format it in the special style that the LLaMA model understands, and get it ready so the assistant can reply."
      ],
      "metadata": {
        "id": "tVmofOhkd8ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "t9l2KS1bdk2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Trying new models**\n",
        "\n",
        "We will now work with 3 models:\n",
        "\n",
        "- Phi3 from Microsoft\n",
        "\n",
        "- Qwen2 from Alibaba Cloud\n",
        "\n",
        "- Starcoder2 from BigCode (ServiceNow + HuggingFace + NVidia)"
      ],
      "metadata": {
        "id": "4NuLq-5td9fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the `PHI3_MODEL_NAME`,`QWEN2_MODEL_NAME` & `STARCODER2_MODEL_NAME`"
      ],
      "metadata": {
        "id": "gsoPHiaTePnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
      ],
      "metadata": {
        "id": "1thake1Wd90l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’re comparing how many different tokenizers **(possibly from different models)** turn text into token IDs and then decode them back to text."
      ],
      "metadata": {
        "id": "p7KExLH-eyHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "tGHTZgo0eOVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a conversation between the user and assistant, format it in the special style that the LLaMA model understands, and get it ready so the assistant can reply."
      ],
      "metadata": {
        "id": "9whVdJa7fQnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "5HwZ2EfwfL1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format a conversation into model-specific prompts using both LLaMA and PHI-3 tokenizers, then print each one to see how they differ in style."
      ],
      "metadata": {
        "id": "QJS2ARpFfEGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "5UMYfjFWe6WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’re comparing how many different tokenizers **(possibly from different models)** turn text into token IDs and then decode them back to text."
      ],
      "metadata": {
        "id": "zTLEFtQKfWS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "print(phi3_tokenizer.encode(text))\n",
        "print()\n",
        "print(qwen2_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "1-36ER8TfWm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing between PHI 3 Tokenizer and Qwen2 Tokenizer"
      ],
      "metadata": {
        "id": "B6HmKqW5fpd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "yCp-0Sqbfp07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the tokenizer for the StarCoder2 model using the model name stored in `STARCODER2_MODEL_NAME`. Allow the tokenizer to run any custom code it needs."
      ],
      "metadata": {
        "id": "xP4q1Xr2f3A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
        "code = \"\"\"\n",
        "def hello_world(person):\n",
        "  print(\"Hello\", person)\n",
        "\"\"\"\n",
        "tokens = starcoder2_tokenizer.encode(code)\n",
        "for token in tokens:\n",
        "  print(f\"{token}={starcoder2_tokenizer.decode(token)}\")"
      ],
      "metadata": {
        "id": "dTCQ5ZvKf3U8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}